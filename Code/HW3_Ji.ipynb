{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIA 6304 Text Mining, Fall 2017\n",
    "## Assignment 3\n",
    "### Stuent:  Leonardo Ji\n",
    "### 9/9/2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**T1.**  Read in or create a data frame with at least one column of text to be analyzed.  This could be the text you used previously or new text. Based on the context of your dataset and the question you want to answer, identify at what processing you think is necessary (stop words, stemming, custom replacement, etc.) Compare the feature space before and after your processing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![R-blogger news site](https://www.r-bloggers.com/wp-content/uploads/2016/04/R_02_2016-05-01.png \"R-blogger\")\n",
    "I want to use R-blogger news site data from web scrapping technique used in the last class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import gzip\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_colwidth', 150) #important for getting all the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# R-Blogger news\n",
    "page = requests.get('https://www.r-bloggers.com')\n",
    "\n",
    "soup = BeautifulSoup(page.text, \"html5lib\")\n",
    "#print(type(soup))\n",
    "#print (soup.prettify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Category</th>\n",
       "      <th>author</th>\n",
       "      <th>Date</th>\n",
       "      <th>link</th>\n",
       "      <th>excerpt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Announcing lime – Explaining the predictions of black-box models</td>\n",
       "      <td>Post</td>\n",
       "      <td>Data Imaginist - R posts</td>\n",
       "      <td>September 13, 2017</td>\n",
       "      <td>https://www.r-bloggers.com/announcing-lime-explaining-the-predictions-of-black-box-models/</td>\n",
       "      <td>I’m very pleased to announce that lime has been released on CRAN. lime is animplementation of the model prediction explanation framework described...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Applying machine learning algorithms – exercises</td>\n",
       "      <td>Post</td>\n",
       "      <td>Euthymios Kasvikis</td>\n",
       "      <td>September 15, 2017</td>\n",
       "      <td>https://www.r-bloggers.com/applying-machine-learning-algorithms-exercises/</td>\n",
       "      <td>INTRODUCTION Dear reader, If you are a newbie in the world of machine learning, then this tutorial is exactly what you need in order to introduce ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Asset Contribution to Portfolio Volatility</td>\n",
       "      <td>Post</td>\n",
       "      <td>R Views</td>\n",
       "      <td>September 12, 2017</td>\n",
       "      <td>https://www.r-bloggers.com/asset-contribution-to-portfolio-volatility/</td>\n",
       "      <td>In our previous portfolio volatility work, we covered how to import stock prices, convert to returns and set weights, calculate portfolio volatili...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Brazilian Yield Curve</td>\n",
       "      <td>Post</td>\n",
       "      <td>R and Finance</td>\n",
       "      <td>September 14, 2017</td>\n",
       "      <td>https://www.r-bloggers.com/brazilian-yield-curve/</td>\n",
       "      <td>The latest version of GetTDData offers function get.yield.curve todownload...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Building rich analytic web apps has never been easier</td>\n",
       "      <td>Post</td>\n",
       "      <td>modern.data</td>\n",
       "      <td>September 13, 2017</td>\n",
       "      <td>https://www.r-bloggers.com/building-rich-analytic-web-apps-has-never-been-easier/</td>\n",
       "      <td>We’re entering a golden age of the “analytics developer.” Cutting edge companies like AirBnB, Apple, and S&amp;P are no longer buying off-the-shelf BI...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           Headline Category  \\\n",
       "0  Announcing lime – Explaining the predictions of black-box models     Post   \n",
       "1                  Applying machine learning algorithms – exercises     Post   \n",
       "2                        Asset Contribution to Portfolio Volatility     Post   \n",
       "3                                             Brazilian Yield Curve     Post   \n",
       "4             Building rich analytic web apps has never been easier     Post   \n",
       "\n",
       "                     author                Date  \\\n",
       "0  Data Imaginist - R posts  September 13, 2017   \n",
       "1        Euthymios Kasvikis  September 15, 2017   \n",
       "2                   R Views  September 12, 2017   \n",
       "3             R and Finance  September 14, 2017   \n",
       "4               modern.data  September 13, 2017   \n",
       "\n",
       "                                                                                         link  \\\n",
       "0  https://www.r-bloggers.com/announcing-lime-explaining-the-predictions-of-black-box-models/   \n",
       "1                  https://www.r-bloggers.com/applying-machine-learning-algorithms-exercises/   \n",
       "2                      https://www.r-bloggers.com/asset-contribution-to-portfolio-volatility/   \n",
       "3                                           https://www.r-bloggers.com/brazilian-yield-curve/   \n",
       "4           https://www.r-bloggers.com/building-rich-analytic-web-apps-has-never-been-easier/   \n",
       "\n",
       "                                                                                                                                                 excerpt  \n",
       "0  I’m very pleased to announce that lime has been released on CRAN. lime is animplementation of the model prediction explanation framework described...  \n",
       "1  INTRODUCTION Dear reader, If you are a newbie in the world of machine learning, then this tutorial is exactly what you need in order to introduce ...  \n",
       "2  In our previous portfolio volatility work, we covered how to import stock prices, convert to returns and set weights, calculate portfolio volatili...  \n",
       "3                                                                          The latest version of GetTDData offers function get.yield.curve todownload...  \n",
       "4  We’re entering a golden age of the “analytics developer.” Cutting edge companies like AirBnB, Apple, and S&P are no longer buying off-the-shelf BI...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BSobj = soup.find_all(\"div\",{\"class\":\"twopost\"})\n",
    "\n",
    "headlines = {}\n",
    "for item in BSobj:\n",
    "    date = item.find(\"div\",{\"class\":\"date\"}).get_text()\n",
    "    link = item.a[\"href\"]\n",
    "    author = item.find(\"a\",{\"rel\":\"author\"}).get_text()\n",
    "    excerpt = item.find(\"p\",{\"class\":\"excerpt\"}).get_text().replace('\\n', '')\n",
    "    headlines[item.a.get_text()] = {}\n",
    "    headlines[item.a.get_text()][\"category\"] = \"Post\"\n",
    "    headlines[item.a.get_text()][\"author\"] = author\n",
    "    headlines[item.a.get_text()][\"date\"] = date\n",
    "    headlines[item.a.get_text()][\"link\"] = link\n",
    "    headlines[item.a.get_text()][\"excerpt\"] = excerpt\n",
    "    \n",
    "#print(headlines)\n",
    "\n",
    "cnndf = pd.DataFrame.from_dict(headlines,orient=\"index\")\n",
    "#print(cnndf.shape)\n",
    "cnndf.reset_index(inplace=True)\n",
    "cnndf.columns = ['Headline', 'Category', 'author', 'Date', 'link', 'excerpt']\n",
    "textStr = cnndf['excerpt'].values\n",
    "cnndf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* I use tf_idf weight, did not normalize value, used english stop words, set min_df to 5%, set max_df to 50%, ngram 1 to 2. It returned 40 feature spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 40)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analysis</th>\n",
       "      <th>analytics</th>\n",
       "      <th>announce</th>\n",
       "      <th>arrived</th>\n",
       "      <th>arrived cran</th>\n",
       "      <th>available</th>\n",
       "      <th>blog</th>\n",
       "      <th>collection</th>\n",
       "      <th>cran</th>\n",
       "      <th>data</th>\n",
       "      <th>...</th>\n",
       "      <th>rstudio</th>\n",
       "      <th>set</th>\n",
       "      <th>time</th>\n",
       "      <th>trust</th>\n",
       "      <th>used</th>\n",
       "      <th>ve</th>\n",
       "      <th>version</th>\n",
       "      <th>week</th>\n",
       "      <th>working</th>\n",
       "      <th>world</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.268684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.757858</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.268684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.981001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.981001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.981001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.268684</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   analysis  analytics  announce  arrived  arrived cran  available  blog  \\\n",
       "0       0.0   0.000000  3.268684      0.0           0.0        0.0   0.0   \n",
       "1       0.0   0.000000  0.000000      0.0           0.0        0.0   0.0   \n",
       "2       0.0   0.000000  0.000000      0.0           0.0        0.0   0.0   \n",
       "3       0.0   0.000000  0.000000      0.0           0.0        0.0   0.0   \n",
       "4       0.0   3.268684  0.000000      0.0           0.0        0.0   0.0   \n",
       "\n",
       "   collection      cran  data    ...     rstudio       set  time     trust  \\\n",
       "0         0.0  2.757858   0.0    ...         0.0  0.000000   0.0  3.268684   \n",
       "1         0.0  0.000000   0.0    ...         0.0  0.000000   0.0  0.000000   \n",
       "2         0.0  0.000000   0.0    ...         0.0  2.981001   0.0  0.000000   \n",
       "3         0.0  0.000000   0.0    ...         0.0  0.000000   0.0  0.000000   \n",
       "4         0.0  0.000000   0.0    ...         0.0  0.000000   0.0  0.000000   \n",
       "\n",
       "   used   ve   version  week  working     world  \n",
       "0   0.0  0.0  0.000000   0.0      0.0  0.000000  \n",
       "1   0.0  0.0  0.000000   0.0      0.0  2.981001  \n",
       "2   0.0  0.0  0.000000   0.0      0.0  0.000000  \n",
       "3   0.0  0.0  2.981001   0.0      0.0  0.000000  \n",
       "4   0.0  0.0  0.000000   0.0      0.0  0.000000  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf1 = TfidfVectorizer(use_idf=True, norm=None, stop_words = \"english\", min_df = 0.05, max_df = 0.5, ngram_range = (1,2)) #define the transformation\n",
    "tf1_dm = tfidf1.fit_transform(textStr) #apply the transformation\n",
    "print(tf1_dm.shape)\n",
    "org_features = tfidf1.get_feature_names()\n",
    "tfidfVectororizerTable = pd.DataFrame(tf1_dm.toarray(), columns = tfidf1.get_feature_names())\n",
    "tfidfVectororizerTable.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If we modify the Sklearn stop words to add two stop words then run the same Tfid Vectorizer we get 38 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318\n",
      "153\n"
     ]
    }
   ],
   "source": [
    "skl_stopwords = text.ENGLISH_STOP_WORDS\n",
    "print(len(skl_stopwords))\n",
    "nltk_stopwords = stopwords.words(\"english\")\n",
    "print(len(nltk_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322\n"
     ]
    }
   ],
   "source": [
    "new_skl_stopwords = set(skl_stopwords)\n",
    "new_skl_stopwords.update(set(['used', 'like', 'need', 'non']))\n",
    "print(len(new_skl_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 36)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analysis</th>\n",
       "      <th>analytics</th>\n",
       "      <th>announce</th>\n",
       "      <th>arrived</th>\n",
       "      <th>arrived cran</th>\n",
       "      <th>available</th>\n",
       "      <th>blog</th>\n",
       "      <th>collection</th>\n",
       "      <th>cran</th>\n",
       "      <th>data</th>\n",
       "      <th>...</th>\n",
       "      <th>release</th>\n",
       "      <th>rstudio</th>\n",
       "      <th>set</th>\n",
       "      <th>time</th>\n",
       "      <th>trust</th>\n",
       "      <th>ve</th>\n",
       "      <th>version</th>\n",
       "      <th>week</th>\n",
       "      <th>working</th>\n",
       "      <th>world</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.268684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.757858</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.268684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.981001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.981001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.981001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.268684</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   analysis  analytics  announce  arrived  arrived cran  available  blog  \\\n",
       "0       0.0   0.000000  3.268684      0.0           0.0        0.0   0.0   \n",
       "1       0.0   0.000000  0.000000      0.0           0.0        0.0   0.0   \n",
       "2       0.0   0.000000  0.000000      0.0           0.0        0.0   0.0   \n",
       "3       0.0   0.000000  0.000000      0.0           0.0        0.0   0.0   \n",
       "4       0.0   3.268684  0.000000      0.0           0.0        0.0   0.0   \n",
       "\n",
       "   collection      cran  data    ...     release  rstudio       set  time  \\\n",
       "0         0.0  2.757858   0.0    ...         0.0      0.0  0.000000   0.0   \n",
       "1         0.0  0.000000   0.0    ...         0.0      0.0  0.000000   0.0   \n",
       "2         0.0  0.000000   0.0    ...         0.0      0.0  2.981001   0.0   \n",
       "3         0.0  0.000000   0.0    ...         0.0      0.0  0.000000   0.0   \n",
       "4         0.0  0.000000   0.0    ...         0.0      0.0  0.000000   0.0   \n",
       "\n",
       "      trust   ve   version  week  working     world  \n",
       "0  3.268684  0.0  0.000000   0.0      0.0  0.000000  \n",
       "1  0.000000  0.0  0.000000   0.0      0.0  2.981001  \n",
       "2  0.000000  0.0  0.000000   0.0      0.0  0.000000  \n",
       "3  0.000000  0.0  2.981001   0.0      0.0  0.000000  \n",
       "4  0.000000  0.0  0.000000   0.0      0.0  0.000000  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf1 = TfidfVectorizer(use_idf=True, norm=None, stop_words = new_skl_stopwords, min_df = 0.05, max_df = 0.5, ngram_range = (1,2)) #define the transformation\n",
    "tf1_dm = tfidf1.fit_transform(textStr) #apply the transformation\n",
    "print(tf1_dm.shape)\n",
    "org_features = tfidf1.get_feature_names()\n",
    "tfidfVectororizerTable = pd.DataFrame(tf1_dm.toarray(), columns = tfidf1.get_feature_names())\n",
    "tfidfVectororizerTable.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If remove two stop words from NLTK stop words then run Tfid Vectorizer we get 45 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n"
     ]
    }
   ],
   "source": [
    "a = set(nltk_stopwords)\n",
    "keepwords = set(['before', 'after'])\n",
    "# remove before and after from NLTK stop words.\n",
    "new_nltk_stopwords = a - keepwords\n",
    "print(len(new_nltk_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the second try I use tf_idf weight, did not normalize value, used modified NLTK stop words, set min_df to 5%, set max_df to 50%, ngram 1 to 2. It returned 45 feature spaces.  Modified NLTK stop words resulted in 5 more features than Sklearn stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 45)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analysis</th>\n",
       "      <th>analytics</th>\n",
       "      <th>announce</th>\n",
       "      <th>around</th>\n",
       "      <th>around world</th>\n",
       "      <th>arrived</th>\n",
       "      <th>arrived cran</th>\n",
       "      <th>available</th>\n",
       "      <th>before</th>\n",
       "      <th>blog</th>\n",
       "      <th>...</th>\n",
       "      <th>release</th>\n",
       "      <th>rstudio</th>\n",
       "      <th>set</th>\n",
       "      <th>time</th>\n",
       "      <th>trust</th>\n",
       "      <th>used</th>\n",
       "      <th>version</th>\n",
       "      <th>week</th>\n",
       "      <th>working</th>\n",
       "      <th>world</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.268684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.268684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.981001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.981001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.981001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.268684</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   analysis  analytics  announce  around  around world  arrived  arrived cran  \\\n",
       "0       0.0   0.000000  3.268684     0.0           0.0      0.0           0.0   \n",
       "1       0.0   0.000000  0.000000     0.0           0.0      0.0           0.0   \n",
       "2       0.0   0.000000  0.000000     0.0           0.0      0.0           0.0   \n",
       "3       0.0   0.000000  0.000000     0.0           0.0      0.0           0.0   \n",
       "4       0.0   3.268684  0.000000     0.0           0.0      0.0           0.0   \n",
       "\n",
       "   available  before  blog    ...     release  rstudio       set  time  \\\n",
       "0        0.0     0.0   0.0    ...         0.0      0.0  0.000000   0.0   \n",
       "1        0.0     0.0   0.0    ...         0.0      0.0  0.000000   0.0   \n",
       "2        0.0     0.0   0.0    ...         0.0      0.0  2.981001   0.0   \n",
       "3        0.0     0.0   0.0    ...         0.0      0.0  0.000000   0.0   \n",
       "4        0.0     0.0   0.0    ...         0.0      0.0  0.000000   0.0   \n",
       "\n",
       "      trust  used   version  week  working     world  \n",
       "0  3.268684   0.0  0.000000   0.0      0.0  0.000000  \n",
       "1  0.000000   0.0  0.000000   0.0      0.0  2.981001  \n",
       "2  0.000000   0.0  0.000000   0.0      0.0  0.000000  \n",
       "3  0.000000   0.0  2.981001   0.0      0.0  0.000000  \n",
       "4  0.000000   0.0  0.000000   0.0      0.0  0.000000  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf2 = TfidfVectorizer(use_idf=True, norm=None, stop_words = new_nltk_stopwords, min_df = 0.05, max_df = 0.5, ngram_range = (1,2)) #define the transformation\n",
    "tf2_dm = tfidf2.fit_transform(textStr) #apply the transformation\n",
    "print(tf2_dm.shape)\n",
    "tfidfVectororizerTable = pd.DataFrame(tf2_dm.toarray(), columns = tfidf2.get_feature_names())\n",
    "tfidfVectororizerTable.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'around',\n",
       " 'around world',\n",
       " 'before',\n",
       " 'everyone',\n",
       " 'get',\n",
       " 'like',\n",
       " 'much',\n",
       " 'need',\n",
       " 'non',\n",
       " 'used'}"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(tfidf2.get_feature_names()) - set(tfidf1.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use Porter Stemmer resulted the same feature spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ps = PorterStemmer() #define method (http://www.nltk.org/api/nltk.stem.html#nltk.stem.porter.PorterStemmer)\n",
    "textStr2 = [ps.stem(word) for word in textStr] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 36)\n"
     ]
    }
   ],
   "source": [
    "tf1_dm = tfidf1.fit_transform(textStr2) #apply the transformation\n",
    "print(tf1_dm.shape)\n",
    "ps_features = tfidf1.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(ps_features))\n",
    "diff = set(org_features) - set(ps_features)\n",
    "print(len(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use Lancaster Stemmer resulted the same feature spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ls = LancasterStemmer()\n",
    "textStr3 = [ls.stem(word) for word in textStr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 36)\n"
     ]
    }
   ],
   "source": [
    "tf1_dm = tfidf1.fit_transform(textStr3) #apply the transformation\n",
    "print(tf1_dm.shape)\n",
    "ls_features = tfidf1.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(ls_features))\n",
    "diff = set(org_features) - set(ls_features)\n",
    "print(len(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use Snowball Stemmer resulted the same feature spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss = SnowballStemmer(\"english\")\n",
    "textStr4 = [ss.stem(word) for word in textStr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 36)\n"
     ]
    }
   ],
   "source": [
    "tf1_dm = tfidf1.fit_transform(textStr4) #apply the transformation\n",
    "print(tf1_dm.shape)\n",
    "ss_features = tfidf1.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(ss_features))\n",
    "diff = set(org_features) - set(ss_features)\n",
    "print(len(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use Lemmatization resulted the same feature spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lemmatization\n",
    "wnl = WordNetLemmatizer()\n",
    "textStr5 = [wnl.lemmatize(word, pos = \"v\") for word in textStr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 36)\n"
     ]
    }
   ],
   "source": [
    "tf1_dm = tfidf1.fit_transform(textStr5) #apply the transformation\n",
    "print(tf1_dm.shape)\n",
    "lemm_features = tfidf1.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(lemm_features))\n",
    "diff = set(org_features) - set(lemm_features)\n",
    "print(len(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define a dictionary of replacement words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using dictionaries\n",
    "news_dict = {'r-studio':'rstudio'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use replace function on words in dictionary on each documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Category</th>\n",
       "      <th>author</th>\n",
       "      <th>Date</th>\n",
       "      <th>link</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>cleantext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Announcing lime – Explaining the predictions of black-box models</td>\n",
       "      <td>Post</td>\n",
       "      <td>Data Imaginist - R posts</td>\n",
       "      <td>September 13, 2017</td>\n",
       "      <td>https://www.r-bloggers.com/announcing-lime-explaining-the-predictions-of-black-box-models/</td>\n",
       "      <td>I’m very pleased to announce that lime has been released on CRAN. lime is animplementation of the model prediction explanation framework described...</td>\n",
       "      <td>i’m very pleased to announce that lime has been released on cran. lime is animplementation of the model prediction explanation framework described...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           Headline Category  \\\n",
       "0  Announcing lime – Explaining the predictions of black-box models     Post   \n",
       "\n",
       "                     author                Date  \\\n",
       "0  Data Imaginist - R posts  September 13, 2017   \n",
       "\n",
       "                                                                                         link  \\\n",
       "0  https://www.r-bloggers.com/announcing-lime-explaining-the-predictions-of-black-box-models/   \n",
       "\n",
       "                                                                                                                                                 excerpt  \\\n",
       "0  I’m very pleased to announce that lime has been released on CRAN. lime is animplementation of the model prediction explanation framework described...   \n",
       "\n",
       "                                                                                                                                               cleantext  \n",
       "0  i’m very pleased to announce that lime has been released on cran. lime is animplementation of the model prediction explanation framework described...  "
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newtext = []\n",
    "for item in cnndf['excerpt']:\n",
    "    for key, value in news_dict.items():\n",
    "        item = item.lower().replace(key.lower(), value)\n",
    "    newtext = newtext + [item]\n",
    "print(len(newtext))\n",
    "cnndf['cleantext'] = newtext\n",
    "cnndf[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use regular expression to to accomplish the same replacement task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multiple_replace(dict, text): \n",
    "\n",
    "  \"\"\" Replace in 'text' all occurences of any key in the given\n",
    "  dictionary by its corresponding value.  Returns the new tring.\"\"\" \n",
    "  text = str(text).lower()\n",
    "\n",
    "  # Create a regular expression  from the dictionary keys\n",
    "  regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dict.keys())))\n",
    "\n",
    "  # For each match, look-up corresponding value in dictionary\n",
    "  return regex.sub(lambda mo: dict[mo.string[mo.start():mo.end()]], text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Category</th>\n",
       "      <th>author</th>\n",
       "      <th>Date</th>\n",
       "      <th>link</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>cleantext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Announcing lime – Explaining the predictions of black-box models</td>\n",
       "      <td>Post</td>\n",
       "      <td>Data Imaginist - R posts</td>\n",
       "      <td>September 13, 2017</td>\n",
       "      <td>https://www.r-bloggers.com/announcing-lime-explaining-the-predictions-of-black-box-models/</td>\n",
       "      <td>I’m very pleased to announce that lime has been released on CRAN. lime is animplementation of the model prediction explanation framework described...</td>\n",
       "      <td>i’m very pleased to announce that lime has been released on cran. lime is animplementation of the model prediction explanation framework described...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           Headline Category  \\\n",
       "0  Announcing lime – Explaining the predictions of black-box models     Post   \n",
       "\n",
       "                     author                Date  \\\n",
       "0  Data Imaginist - R posts  September 13, 2017   \n",
       "\n",
       "                                                                                         link  \\\n",
       "0  https://www.r-bloggers.com/announcing-lime-explaining-the-predictions-of-black-box-models/   \n",
       "\n",
       "                                                                                                                                                 excerpt  \\\n",
       "0  I’m very pleased to announce that lime has been released on CRAN. lime is animplementation of the model prediction explanation framework described...   \n",
       "\n",
       "                                                                                                                                               cleantext  \n",
       "0  i’m very pleased to announce that lime has been released on cran. lime is animplementation of the model prediction explanation framework described...  "
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnndf['cleantext'] = cnndf['excerpt'].apply(lambda x: multiple_replace(news_dict, x))\n",
    "cnndf[0:1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom replacement did not change the number of feature space but it include r-studio and rstudio in the same feature weight number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 36)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf1_dm = tfidf1.fit_transform(cnndf['cleantext']) #apply the transformation\n",
    "print(tf1_dm.shape)\n",
    "clean_text_features = tfidf1.get_feature_names()\n",
    "diff = set(clean_text_features) - set(org_features)  \n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analysis</th>\n",
       "      <th>analytics</th>\n",
       "      <th>announce</th>\n",
       "      <th>arrived</th>\n",
       "      <th>arrived cran</th>\n",
       "      <th>available</th>\n",
       "      <th>blog</th>\n",
       "      <th>collection</th>\n",
       "      <th>cran</th>\n",
       "      <th>data</th>\n",
       "      <th>...</th>\n",
       "      <th>release</th>\n",
       "      <th>rstudio</th>\n",
       "      <th>set</th>\n",
       "      <th>time</th>\n",
       "      <th>trust</th>\n",
       "      <th>ve</th>\n",
       "      <th>version</th>\n",
       "      <th>week</th>\n",
       "      <th>working</th>\n",
       "      <th>world</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.268684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.757858</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.268684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.981001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.981001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.981001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.268684</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   analysis  analytics  announce  arrived  arrived cran  available  blog  \\\n",
       "0       0.0   0.000000  3.268684      0.0           0.0        0.0   0.0   \n",
       "1       0.0   0.000000  0.000000      0.0           0.0        0.0   0.0   \n",
       "2       0.0   0.000000  0.000000      0.0           0.0        0.0   0.0   \n",
       "3       0.0   0.000000  0.000000      0.0           0.0        0.0   0.0   \n",
       "4       0.0   3.268684  0.000000      0.0           0.0        0.0   0.0   \n",
       "\n",
       "   collection      cran  data    ...     release  rstudio       set  time  \\\n",
       "0         0.0  2.757858   0.0    ...         0.0      0.0  0.000000   0.0   \n",
       "1         0.0  0.000000   0.0    ...         0.0      0.0  0.000000   0.0   \n",
       "2         0.0  0.000000   0.0    ...         0.0      0.0  2.981001   0.0   \n",
       "3         0.0  0.000000   0.0    ...         0.0      0.0  0.000000   0.0   \n",
       "4         0.0  0.000000   0.0    ...         0.0      0.0  0.000000   0.0   \n",
       "\n",
       "      trust   ve   version  week  working     world  \n",
       "0  3.268684  0.0  0.000000   0.0      0.0  0.000000  \n",
       "1  0.000000  0.0  0.000000   0.0      0.0  2.981001  \n",
       "2  0.000000  0.0  0.000000   0.0      0.0  0.000000  \n",
       "3  0.000000  0.0  2.981001   0.0      0.0  0.000000  \n",
       "4  0.000000  0.0  0.000000   0.0      0.0  0.000000  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfVectororizerTable = pd.DataFrame(tf1_dm.toarray(), columns = tfidf1.get_feature_names())\n",
    "tfidfVectororizerTable.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1**. Write a short description of the context of the dataset in your own words. Make sure your answer is no longer than three paragraphs, and should at minimum answer these questions:\n",
    "* Why did you choose the processing that you did? Give several specific examples. \n",
    "* What is the effect of the replacement on your feature space?  Does this make sense? Is it helpful for answering your question?  Why or why not?\n",
    "Audience: technical – fellow data scientists or other technical staff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I collected twenty-eight blog posts from R-blogger site. The dataset contains five columns: category, author, date, link, excerpt. The excerpt column contains excerpt text that I can create a vector space model. Vector space model convert text documents into numeric model such as a sparse matrix. The Sklearn Learn library vector space function allow me to transform blog posts to a sparse matrix of documents (rows) and features/words (columns).\n",
    "\n",
    "With all other parameters the same I used Sklearn stop words, a modified version of Sklearn stop words, and modified version of NLTK stop words to create vector space model. Using Sklearn stop words the vector space model contains 40 feature spaces.  Then I added four stop words \"used\", \"like\", \"need\", \"non\" to the Sklarn stop words set, run the same vector space model, and the feature space number is 36. Using NLTK stop words I actually got 45 feature spaces. Then I compare the difference and see the NLTK stop words list are smaller and it did not filter out some words like \"around\", \"everyone\", \"much\"...etc. Next I applied porter stemmer, Lancaster stemmer, Snowball stemmer, Lemmatization to the text before create vector space model using modified Sklearn stop words.  The results from all four techniques did not change still 36 feature spaces.  Lastly I used regular expression to replace word \"r-studio\" to \"rstudio\".\n",
    "\n",
    "This change make sense because the question I want to ask is which blog post to read to learn r-studio verse all other blog posts. Because \"r-studio\" and \"rstudio\" are the same thing I want to add the document frequency to the same feature space, and to the same tfidf weight number. In this case the effect of this replacement did not change the feature space or the \"rstudio\" column weights, but it makes sense to do this replacement knowing \"r-studio\" and \"rstudio\" are referring the same thing and in case in text both these terms occur we want to group them together.  \"r-studio\" is an R language IDE software. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**T2.** Create a sentiment dictionary from one of the sources in class or find/create your own (potential bonus points for appropriate creativity). Using your dictionary, create sentiment labels for the text entries in your corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SNAP](https://snap.stanford.edu/images/snap_logo.png \"Stanford Network Analysis Project\")\n",
    "I want to use a Amazon grocery and gourmet food review data from [SNAP](https://snap.stanford.edu/data/web-Amazon.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151254\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1VEELTKS8NLZB</td>\n",
       "      <td>616719923X</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Just another flavor of Kit Kat but the taste is unique and a bit different.  The only thing that is bothersome is the price.  I thought it was a b...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Good Taste</td>\n",
       "      <td>1370044800</td>\n",
       "      <td>06 1, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A14R9XMZVJ6INB</td>\n",
       "      <td>616719923X</td>\n",
       "      <td>amf0001</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>I bought this on impulse and it comes from Japan,  which amused my family,  all those weird stamps and markings on the package. So that was fun.  ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5 stars,  sadly not as wonderful as I had hoped</td>\n",
       "      <td>1400457600</td>\n",
       "      <td>05 19, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A27IQHDZFQFNGG</td>\n",
       "      <td>616719923X</td>\n",
       "      <td>Caitlin</td>\n",
       "      <td>[3, 4]</td>\n",
       "      <td>Really good. Great gift for any fan of green tea! Just so expensive to purchase candy from across the sea.</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Yum!</td>\n",
       "      <td>1381190400</td>\n",
       "      <td>10 8, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A31QY5TASILE89</td>\n",
       "      <td>616719923X</td>\n",
       "      <td>DebraDownSth</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I had never had it before, was curious to see what it was like. Smooth, great subtle good flavor. I am ordering more and plan to make it a routine.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Unexpected flavor meld</td>\n",
       "      <td>1369008000</td>\n",
       "      <td>05 20, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A2LWK003FFMCI5</td>\n",
       "      <td>616719923X</td>\n",
       "      <td>Diana X.</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>I've been looking forward to trying these after hearing about how popular they were in Japan, and among Kit Kat fans as well. I do not recommend o...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Not a very strong tea flavor, but still yummy &amp; creamy!</td>\n",
       "      <td>1369526400</td>\n",
       "      <td>05 26, 2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin     reviewerName helpful  \\\n",
       "0  A1VEELTKS8NLZB  616719923X  Amazon Customer  [0, 0]   \n",
       "1  A14R9XMZVJ6INB  616719923X          amf0001  [0, 1]   \n",
       "2  A27IQHDZFQFNGG  616719923X          Caitlin  [3, 4]   \n",
       "3  A31QY5TASILE89  616719923X     DebraDownSth  [0, 0]   \n",
       "4  A2LWK003FFMCI5  616719923X         Diana X.  [1, 2]   \n",
       "\n",
       "                                                                                                                                              reviewText  \\\n",
       "0  Just another flavor of Kit Kat but the taste is unique and a bit different.  The only thing that is bothersome is the price.  I thought it was a b...   \n",
       "1  I bought this on impulse and it comes from Japan,  which amused my family,  all those weird stamps and markings on the package. So that was fun.  ...   \n",
       "2                                             Really good. Great gift for any fan of green tea! Just so expensive to purchase candy from across the sea.   \n",
       "3    I had never had it before, was curious to see what it was like. Smooth, great subtle good flavor. I am ordering more and plan to make it a routine.   \n",
       "4  I've been looking forward to trying these after hearing about how popular they were in Japan, and among Kit Kat fans as well. I do not recommend o...   \n",
       "\n",
       "   overall                                                  summary  \\\n",
       "0      4.0                                               Good Taste   \n",
       "1      3.0        3.5 stars,  sadly not as wonderful as I had hoped   \n",
       "2      4.0                                                     Yum!   \n",
       "3      5.0                                   Unexpected flavor meld   \n",
       "4      4.0  Not a very strong tea flavor, but still yummy & creamy!   \n",
       "\n",
       "   unixReviewTime   reviewTime  \n",
       "0      1370044800   06 1, 2013  \n",
       "1      1400457600  05 19, 2014  \n",
       "2      1381190400   10 8, 2013  \n",
       "3      1369008000  05 20, 2013  \n",
       "4      1369526400  05 26, 2013  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "amazonFood = getDF('reviews_Grocery_and_Gourmet_Food_5.json.gz')\n",
    "print(len(amazonFood))\n",
    "amazonFood.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [AFINN Dictionary](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> 2477\n"
     ]
    }
   ],
   "source": [
    "#some dictionaries assign a value \n",
    "afinn = {}\n",
    "for line in open(\"AFINN-111.txt\"):\n",
    "    tt = line.split('\\t')\n",
    "    afinn.update({tt[0]:int(tt[1])})\n",
    "print(type(afinn), len(afinn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#amplification and negation words from qdap\n",
    "negate = [\"aint\", \"arent\",\"cant\", \"couldnt\" , \"didnt\" , \"doesnt\" ,\"dont\" ,\"hasnt\" , \"isnt\" ,\"mightnt\" , \"mustnt\" ,\"neither\" ,\"never\", \"no\" ,\"nobody\" , \"nor\", \"not\" , \"shant\", \"shouldnt\", \"wasnt\" , \"werent\" ,\"wont\", \"wouldnt\"]\n",
    "amplify = [\"acute\" ,\"acutely\", \"certain\", \"certainly\" ,\"colossal\", \"colossally\",\"deep\" , \"deeply\" , \"definite\",\"definitely\" ,\"enormous\",\"enormously\" , \"extreme\", \"extremely\" ,\"great\",\"greatly\" ,\"heavily\", \"heavy\", \"high\",\"highly\" ,\"huge\",\"hugely\" , \"immense\", \"immensely\" ,\"incalculable\" ,\"incalculably\",\"massive\", \"massively\", \"more\",\"particular\" ,\"particularly\",\"purpose\", \"purposely\", \"quite\" ,\"real\" ,\"really\",\"serious\", \"seriously\", \"severe\",\"severely\" ,\"significant\" ,\"significantly\",\"sure\",\"surely\" , \"true\" ,\"truly\" ,\"vast\" , \"vastly\" , \"very\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AFINN function to use amplification and negation words from qdap.  The previous amplification doubles the sentiment score while negation reverse the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def afinn_sent(inputstring):\n",
    "    sentcount =0\n",
    "    i=0\n",
    "    for word in inputstring.split():\n",
    "        prev = inputstring.split().pop(i-1)\n",
    "        if word in afinn:\n",
    "            if negate.count(prev):\n",
    "                # negate positive words means negative sentiment score\n",
    "                sentcount -= afinn[word]\n",
    "            elif amplify.count(prev):\n",
    "                # amplification double positive sentiment score\n",
    "                sentcount += afinn[word]*2\n",
    "            else:\n",
    "                sentcount += afinn[word]\n",
    "        i+=1\n",
    "    \n",
    "    if (sentcount < 0):\n",
    "        sentiment = 'Negative'\n",
    "    elif (sentcount > 0):\n",
    "        sentiment = 'Positive'\n",
    "    else:\n",
    "        sentiment = 'Neutral'\n",
    "    \n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>afinn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just another flavor of Kit Kat but the taste is unique and a bit different.  The only thing that is bothersome is the price.  I thought it was a b...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I bought this on impulse and it comes from Japan,  which amused my family,  all those weird stamps and markings on the package. So that was fun.  ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Really good. Great gift for any fan of green tea! Just so expensive to purchase candy from across the sea.</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I had never had it before, was curious to see what it was like. Smooth, great subtle good flavor. I am ordering more and plan to make it a routine.</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I've been looking forward to trying these after hearing about how popular they were in Japan, and among Kit Kat fans as well. I do not recommend o...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                              reviewText  \\\n",
       "0  Just another flavor of Kit Kat but the taste is unique and a bit different.  The only thing that is bothersome is the price.  I thought it was a b...   \n",
       "1  I bought this on impulse and it comes from Japan,  which amused my family,  all those weird stamps and markings on the package. So that was fun.  ...   \n",
       "2                                             Really good. Great gift for any fan of green tea! Just so expensive to purchase candy from across the sea.   \n",
       "3    I had never had it before, was curious to see what it was like. Smooth, great subtle good flavor. I am ordering more and plan to make it a routine.   \n",
       "4  I've been looking forward to trying these after hearing about how popular they were in Japan, and among Kit Kat fans as well. I do not recommend o...   \n",
       "\n",
       "      afinn  \n",
       "0  Negative  \n",
       "1  Positive  \n",
       "2  Positive  \n",
       "3  Positive  \n",
       "4  Positive  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazonFood['afinn'] = amazonFood.reviewText.apply(lambda x: afinn_sent(x.lower()))\n",
    "amazonFood[['reviewText','afinn']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [UIC Bing Liu Sentiment Lexicon](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HL pos  size: 2006\n",
      "HL neg  size: 4783\n"
     ]
    }
   ],
   "source": [
    "#some sort into buckets\n",
    "HLpos = [line.strip() for line in  open('HLpos.txt','r')]\n",
    "HLneg = [line.strip() for line in  open('HLneg.txt','r',encoding = 'latin-1')]\n",
    "print(\"HL pos  size: \" + str(len(HLpos)))\n",
    "print(\"HL neg  size: \" + str(len(HLneg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HL function also uses amplification and negation words from qdap to calculates the sentiment.\n",
    "![HL Sentiment Formula](HLFormula.png \"HLFormula\")\n",
    "$$Sentiment = \\frac{(positiveCount - negativeCount)}{(positiveCount + negativeCount)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hl_sent(inputstring):\n",
    "\n",
    "    poscount = 0\n",
    "    negcount = 0\n",
    "    i = 0\n",
    "\n",
    "    for word in inputstring.split():\n",
    "        if i > 0:\n",
    "            prev = inputstring.split().pop(i-1)\n",
    "        else:\n",
    "            prev =\"\"\n",
    "\n",
    "        if HLpos.count(word):\n",
    "            if negate.count(prev):\n",
    "                negcount += 1\n",
    "            elif amplify.count(prev):\n",
    "                poscount +=2\n",
    "            else: \n",
    "                poscount +=1\n",
    "        elif HLneg.count(word):\n",
    "            if negate.count(prev):\n",
    "                poscount += 1\n",
    "            elif amplify.count(prev):\n",
    "                negcount +=2\n",
    "            else:\n",
    "                negcount +=1\n",
    "        i+=1\n",
    "    \n",
    "    if poscount+negcount > 0:\n",
    "        t = float((poscount - negcount)/(poscount+negcount))\n",
    "        \n",
    "    else:\n",
    "        t = 0\n",
    "    \n",
    "    if t > 0:\n",
    "        tone = \"Positive\"\n",
    "    elif t < 0:\n",
    "        tone = \"Negative\"\n",
    "    else:\n",
    "        tone = \"Neutral\"\n",
    "    \n",
    "    return tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>afinn</th>\n",
       "      <th>hl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just another flavor of Kit Kat but the taste is unique and a bit different.  The only thing that is bothersome is the price.  I thought it was a b...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I bought this on impulse and it comes from Japan,  which amused my family,  all those weird stamps and markings on the package. So that was fun.  ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Really good. Great gift for any fan of green tea! Just so expensive to purchase candy from across the sea.</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I had never had it before, was curious to see what it was like. Smooth, great subtle good flavor. I am ordering more and plan to make it a routine.</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I've been looking forward to trying these after hearing about how popular they were in Japan, and among Kit Kat fans as well. I do not recommend o...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                              reviewText  \\\n",
       "0  Just another flavor of Kit Kat but the taste is unique and a bit different.  The only thing that is bothersome is the price.  I thought it was a b...   \n",
       "1  I bought this on impulse and it comes from Japan,  which amused my family,  all those weird stamps and markings on the package. So that was fun.  ...   \n",
       "2                                             Really good. Great gift for any fan of green tea! Just so expensive to purchase candy from across the sea.   \n",
       "3    I had never had it before, was curious to see what it was like. Smooth, great subtle good flavor. I am ordering more and plan to make it a routine.   \n",
       "4  I've been looking forward to trying these after hearing about how popular they were in Japan, and among Kit Kat fans as well. I do not recommend o...   \n",
       "\n",
       "      afinn        hl  \n",
       "0  Negative  Negative  \n",
       "1  Positive   Neutral  \n",
       "2  Positive   Neutral  \n",
       "3  Positive  Positive  \n",
       "4  Positive  Positive  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazonFood['hl'] = amazonFood.reviewText.apply(lambda x: hl_sent(x.lower()))\n",
    "amazonFood[['reviewText','afinn','hl']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [NLTK Vader Lexicon](http://www.nltk.org/howto/sentiment.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vader_sent(inputstring):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    ss = sid.polarity_scores(inputstring)\n",
    "    #for k in sorted(ss):\n",
    "    #    print('{0}: {1}, '.format(k, ss[k]), end='')\n",
    "    tone = 'Neutral'\n",
    "    t = ss['pos'] - ss['neg']\n",
    "    if ss['neu'] > .5:\n",
    "        tone = 'Neutral'\n",
    "    elif t > 0:\n",
    "        tone = \"Positive\"\n",
    "    elif t < 0:\n",
    "        tone = \"Negative\"\n",
    "    return tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>afinn</th>\n",
       "      <th>hl</th>\n",
       "      <th>vader</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just another flavor of Kit Kat but the taste is unique and a bit different.  The only thing that is bothersome is the price.  I thought it was a b...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I bought this on impulse and it comes from Japan,  which amused my family,  all those weird stamps and markings on the package. So that was fun.  ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Really good. Great gift for any fan of green tea! Just so expensive to purchase candy from across the sea.</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I had never had it before, was curious to see what it was like. Smooth, great subtle good flavor. I am ordering more and plan to make it a routine.</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I've been looking forward to trying these after hearing about how popular they were in Japan, and among Kit Kat fans as well. I do not recommend o...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                              reviewText  \\\n",
       "0  Just another flavor of Kit Kat but the taste is unique and a bit different.  The only thing that is bothersome is the price.  I thought it was a b...   \n",
       "1  I bought this on impulse and it comes from Japan,  which amused my family,  all those weird stamps and markings on the package. So that was fun.  ...   \n",
       "2                                             Really good. Great gift for any fan of green tea! Just so expensive to purchase candy from across the sea.   \n",
       "3    I had never had it before, was curious to see what it was like. Smooth, great subtle good flavor. I am ordering more and plan to make it a routine.   \n",
       "4  I've been looking forward to trying these after hearing about how popular they were in Japan, and among Kit Kat fans as well. I do not recommend o...   \n",
       "\n",
       "      afinn        hl    vader  \n",
       "0  Negative  Negative  Neutral  \n",
       "1  Positive   Neutral  Neutral  \n",
       "2  Positive   Neutral  Neutral  \n",
       "3  Positive  Positive  Neutral  \n",
       "4  Positive  Positive  Neutral  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazonFood['vader'] = amazonFood.reviewText.apply(lambda x: vader_sent(x.lower()))\n",
    "amazonFood[['reviewText','afinn','hl','vader']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save and read to binary file so I donnot have to run again.\n",
    "amazonFood.to_pickle(\"amazonFood.bin\")\n",
    "#amazonFood.to_csv(\"amazonFood.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amazonFood = pd.read_pickle('amazonFood.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* I used a voting system using previous three sentiment results to get final results.  This is like a ensemble model to combine three methods to reduce variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vote_sent(amazonFood):\n",
    "    vote = [] \n",
    "    for index, row in amazonFood.iterrows():\n",
    "        t = 0\n",
    "        if (row['afinn'] == 'Positive'):\n",
    "            t += 1\n",
    "        elif (row['afinn'] == 'Negative'):\n",
    "            t -=1\n",
    "        \n",
    "        if (row['hl'] == 'Positive'):\n",
    "            t += 1\n",
    "        elif (row['hl'] == 'Negative'):\n",
    "            t -=1    \n",
    "\n",
    "        if (row['vader'] == 'Positive'):\n",
    "            t += 1\n",
    "        elif (row['vader'] == 'Negative'):\n",
    "            t -=1\n",
    "    \n",
    "        if t > 0:\n",
    "            vote.append('Positive')\n",
    "        elif t < 0:\n",
    "            vote.append('Negative')\n",
    "        else:\n",
    "            vote.append('Neutral')\n",
    "\n",
    "    amazonFood['vote'] = vote\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>afinn</th>\n",
       "      <th>hl</th>\n",
       "      <th>vader</th>\n",
       "      <th>vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just another flavor of Kit Kat but the taste is unique and a bit different.  The only thing that is bothersome is the price.  I thought it was a b...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I bought this on impulse and it comes from Japan,  which amused my family,  all those weird stamps and markings on the package. So that was fun.  ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Really good. Great gift for any fan of green tea! Just so expensive to purchase candy from across the sea.</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I had never had it before, was curious to see what it was like. Smooth, great subtle good flavor. I am ordering more and plan to make it a routine.</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I've been looking forward to trying these after hearing about how popular they were in Japan, and among Kit Kat fans as well. I do not recommend o...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                              reviewText  \\\n",
       "0  Just another flavor of Kit Kat but the taste is unique and a bit different.  The only thing that is bothersome is the price.  I thought it was a b...   \n",
       "1  I bought this on impulse and it comes from Japan,  which amused my family,  all those weird stamps and markings on the package. So that was fun.  ...   \n",
       "2                                             Really good. Great gift for any fan of green tea! Just so expensive to purchase candy from across the sea.   \n",
       "3    I had never had it before, was curious to see what it was like. Smooth, great subtle good flavor. I am ordering more and plan to make it a routine.   \n",
       "4  I've been looking forward to trying these after hearing about how popular they were in Japan, and among Kit Kat fans as well. I do not recommend o...   \n",
       "\n",
       "      afinn        hl    vader      vote  \n",
       "0  Negative  Negative  Neutral  Negative  \n",
       "1  Positive   Neutral  Neutral  Positive  \n",
       "2  Positive   Neutral  Neutral  Positive  \n",
       "3  Positive  Positive  Neutral  Positive  \n",
       "4  Positive  Positive  Neutral  Positive  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vote_sent(amazonFood)\n",
    "amazonFood[['reviewText','afinn','hl','vader','vote']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10465\n",
      "0.0691882528726513\n"
     ]
    }
   ],
   "source": [
    "ratingDoesnotMatch = amazonFood[(amazonFood['overall'] == 5) & ('Positive' != amazonFood['hl'])]\n",
    "print(len(ratingDoesnotMatch))\n",
    "print(len(ratingDoesnotMatch)/len(amazonFood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9907\n",
      "0.06549909423883005\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>afinn</th>\n",
       "      <th>hl</th>\n",
       "      <th>vader</th>\n",
       "      <th>vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>A3MLKJ1HHA2SX0</td>\n",
       "      <td>B0000531B7</td>\n",
       "      <td>Robert L. Stinnett</td>\n",
       "      <td>[5, 6]</td>\n",
       "      <td>If you are a fan of peanut butter and chocolate, than this bar is for you.  I've tried a number of energy bars and they all suffer from one proble...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Best Energy Bar Out There</td>\n",
       "      <td>1162598400</td>\n",
       "      <td>11 4, 2006</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>A2B7BUH8834Y6M</td>\n",
       "      <td>B0000537AF</td>\n",
       "      <td>Shelley Gammon \"Geek\"</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>I love having one of these in my backpack or purse as a back-up pick-me up that isn't overloaded with sugar, sodium or artificial sweeteners. I'm ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>My favorite protein bar</td>\n",
       "      <td>1341273600</td>\n",
       "      <td>07 3, 2012</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>A3NEAETOSXDBOM</td>\n",
       "      <td>B0000537AF</td>\n",
       "      <td>Stephen M. Charme</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>I am a runner and discovered this in a grocery store when I was at the four mile mark of a ten mile run and stopped for some water. This tastes gr...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Tastes great and gives energy</td>\n",
       "      <td>1219622400</td>\n",
       "      <td>08 25, 2008</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>A2BB4DGBRVGKWW</td>\n",
       "      <td>B00005C2M2</td>\n",
       "      <td>A Customer \"customer12345657\"</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>This is awesome.  It tastes like the fake marshmallows they put in immoral breakfast cereal, but instead of just tasting like sugar it tastes like...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Mmmmmmmm</td>\n",
       "      <td>1351987200</td>\n",
       "      <td>11 4, 2012</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>A3OFJES10PZGEH</td>\n",
       "      <td>B00005C2M2</td>\n",
       "      <td>SN</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>It's cracked to pieces on purpose so it's easier to eat.  It's a bit sweeter than regular ice cream because there's no water.  Convenient to take ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Great Taste! Convenient! Sweet!</td>\n",
       "      <td>1312848000</td>\n",
       "      <td>08 9, 2011</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        reviewerID        asin                   reviewerName helpful  \\\n",
       "44  A3MLKJ1HHA2SX0  B0000531B7             Robert L. Stinnett  [5, 6]   \n",
       "58  A2B7BUH8834Y6M  B0000537AF          Shelley Gammon \"Geek\"  [1, 1]   \n",
       "59  A3NEAETOSXDBOM  B0000537AF              Stephen M. Charme  [1, 1]   \n",
       "60  A2BB4DGBRVGKWW  B00005C2M2  A Customer \"customer12345657\"  [1, 2]   \n",
       "66  A3OFJES10PZGEH  B00005C2M2                             SN  [0, 0]   \n",
       "\n",
       "                                                                                                                                               reviewText  \\\n",
       "44  If you are a fan of peanut butter and chocolate, than this bar is for you.  I've tried a number of energy bars and they all suffer from one proble...   \n",
       "58  I love having one of these in my backpack or purse as a back-up pick-me up that isn't overloaded with sugar, sodium or artificial sweeteners. I'm ...   \n",
       "59  I am a runner and discovered this in a grocery store when I was at the four mile mark of a ten mile run and stopped for some water. This tastes gr...   \n",
       "60  This is awesome.  It tastes like the fake marshmallows they put in immoral breakfast cereal, but instead of just tasting like sugar it tastes like...   \n",
       "66  It's cracked to pieces on purpose so it's easier to eat.  It's a bit sweeter than regular ice cream because there's no water.  Convenient to take ...   \n",
       "\n",
       "    overall                          summary  unixReviewTime   reviewTime  \\\n",
       "44      5.0        Best Energy Bar Out There      1162598400   11 4, 2006   \n",
       "58      5.0          My favorite protein bar      1341273600   07 3, 2012   \n",
       "59      5.0    Tastes great and gives energy      1219622400  08 25, 2008   \n",
       "60      5.0                         Mmmmmmmm      1351987200   11 4, 2012   \n",
       "66      5.0  Great Taste! Convenient! Sweet!      1312848000   08 9, 2011   \n",
       "\n",
       "       afinn        hl    vader      vote  \n",
       "44  Negative  Negative  Neutral  Negative  \n",
       "58  Negative  Positive  Neutral   Neutral  \n",
       "59  Negative  Positive  Neutral   Neutral  \n",
       "60  Positive  Negative  Neutral   Neutral  \n",
       "66  Positive  Positive  Neutral  Negative  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingDoesnotMatch = amazonFood[(amazonFood['overall'] == 5) & ('Positive' != amazonFood['vote'])]\n",
    "print(len(ratingDoesnotMatch))\n",
    "print(len(ratingDoesnotMatch)/len(amazonFood))\n",
    "ratingDoesnotMatch.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067\n",
      "0.026888545096328032\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>afinn</th>\n",
       "      <th>hl</th>\n",
       "      <th>vader</th>\n",
       "      <th>vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A33NXNZ79H5K51</td>\n",
       "      <td>616719923X</td>\n",
       "      <td>Jean M \"JM\"</td>\n",
       "      <td>[0, 10]</td>\n",
       "      <td>I love green tea, I love Kit Kats, but the two do not belong together. I hate the after taste of them.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Not a fan</td>\n",
       "      <td>1348012800</td>\n",
       "      <td>09 19, 2012</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>A14YSMLYLJEMET</td>\n",
       "      <td>B00004S1C5</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>[8, 11]</td>\n",
       "      <td>This product is no where near natural / organic-I only wish I had seen the other reviews before purchasing! It contains all the things I did not w...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Not natural/organic at all</td>\n",
       "      <td>1364515200</td>\n",
       "      <td>03 29, 2013</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>A3OH4OZFZGEH75</td>\n",
       "      <td>B0000CCZYY</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>[1, 4]</td>\n",
       "      <td>Licorice is my favorite candy, and it promotes good digestion. Since I have some digestive issues, I eat licorice on a regular basis. I've tried b...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Not soft at all. Basically same as cheap licorice but with stronger flavor.</td>\n",
       "      <td>1365120000</td>\n",
       "      <td>04 5, 2013</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>A2OUNVRPRWH0</td>\n",
       "      <td>B0000CCZYY</td>\n",
       "      <td>The Kittie \"Kittie\"</td>\n",
       "      <td>[6, 11]</td>\n",
       "      <td>This is an awesome product, natural, not a lot of ingredients, good flavor, not too sweet.But I no longer love our scaredy-cat friends at Amazon! ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Love this candy!</td>\n",
       "      <td>1367884800</td>\n",
       "      <td>05 7, 2013</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>AX04H2SPKO02S</td>\n",
       "      <td>B0000CD06J</td>\n",
       "      <td>J. Wang \"jyswang\"</td>\n",
       "      <td>[0, 3]</td>\n",
       "      <td>As soon as I had a couple of sips, my eczema started uncontrollably itching. That's one of the signs when something I eat is NOT gluten-free. Will...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NOT gluten free</td>\n",
       "      <td>1362528000</td>\n",
       "      <td>03 6, 2013</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        reviewerID        asin         reviewerName  helpful  \\\n",
       "9   A33NXNZ79H5K51  616719923X          Jean M \"JM\"  [0, 10]   \n",
       "32  A14YSMLYLJEMET  B00004S1C5      Amazon Customer  [8, 11]   \n",
       "75  A3OH4OZFZGEH75  B0000CCZYY      Amazon Customer   [1, 4]   \n",
       "82    A2OUNVRPRWH0  B0000CCZYY  The Kittie \"Kittie\"  [6, 11]   \n",
       "85   AX04H2SPKO02S  B0000CD06J    J. Wang \"jyswang\"   [0, 3]   \n",
       "\n",
       "                                                                                                                                               reviewText  \\\n",
       "9                                                  I love green tea, I love Kit Kats, but the two do not belong together. I hate the after taste of them.   \n",
       "32  This product is no where near natural / organic-I only wish I had seen the other reviews before purchasing! It contains all the things I did not w...   \n",
       "75  Licorice is my favorite candy, and it promotes good digestion. Since I have some digestive issues, I eat licorice on a regular basis. I've tried b...   \n",
       "82  This is an awesome product, natural, not a lot of ingredients, good flavor, not too sweet.But I no longer love our scaredy-cat friends at Amazon! ...   \n",
       "85  As soon as I had a couple of sips, my eczema started uncontrollably itching. That's one of the signs when something I eat is NOT gluten-free. Will...   \n",
       "\n",
       "    overall  \\\n",
       "9       1.0   \n",
       "32      1.0   \n",
       "75      1.0   \n",
       "82      1.0   \n",
       "85      1.0   \n",
       "\n",
       "                                                                        summary  \\\n",
       "9                                                                     Not a fan   \n",
       "32                                                   Not natural/organic at all   \n",
       "75  Not soft at all. Basically same as cheap licorice but with stronger flavor.   \n",
       "82                                                             Love this candy!   \n",
       "85                                                              NOT gluten free   \n",
       "\n",
       "    unixReviewTime   reviewTime     afinn        hl    vader      vote  \n",
       "9       1348012800  09 19, 2012  Positive  Positive  Neutral  Positive  \n",
       "32      1364515200  03 29, 2013  Positive  Positive  Neutral  Positive  \n",
       "75      1365120000   04 5, 2013  Positive  Positive  Neutral  Positive  \n",
       "82      1367884800   05 7, 2013  Positive  Positive  Neutral  Positive  \n",
       "85      1362528000   03 6, 2013   Neutral   Neutral  Neutral   Neutral  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingDoesnotMatch = amazonFood[(amazonFood['overall'] == 1) & ('Negative' != amazonFood['vote'])]\n",
    "print(len(ratingDoesnotMatch))\n",
    "print(len(ratingDoesnotMatch)/len(amazonFood))\n",
    "ratingDoesnotMatch.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>afinn</th>\n",
       "      <th>hl</th>\n",
       "      <th>vader</th>\n",
       "      <th>vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>AY8UH4COUYMGZ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This bottle is a steal, at Whole Foods (at least where I live) a bottle half this size costs twice as much. Before I was also using imitation vani...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID  overall  \\\n",
       "96  AY8UH4COUYMGZ      5.0   \n",
       "\n",
       "                                                                                                                                               reviewText  \\\n",
       "96  This bottle is a steal, at Whole Foods (at least where I live) a bottle half this size costs twice as much. Before I was also using imitation vani...   \n",
       "\n",
       "       afinn        hl    vader     vote  \n",
       "96  Positive  Negative  Neutral  Neutral  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazonFood.iloc[96:97][['reviewerID','overall','reviewText','afinn','hl','vader','vote']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.** Write a short description of how the sentiment analysis was done and what the outcome is. Make sure your answer is no longer than three paragraphs, and should at minimum answer these questions:\n",
    "* How did your processing affect the sentiment assignment, if at all?\n",
    "* What measure did you use to determine the sentiment label?  Why?  Do any of the label assignments surprise you? \n",
    "* Include a few specific examples of label assignment and how it was determined and why it does or does not make sense.\n",
    "Audience: general – management or non-technical staff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to use a different dataset for this question: Amazon grocery and gourmet food review data gathered by Stanford Network Analysis Project (SNAP). This review dataset contains 151,254 user reviews from May 1996 to July 2014.  Dictionary are basically list of words and a sentiment score on each word (positive or negative).   For each document, first I converted text to lower cases, then I matches the positive or negative words from dictionary word list to sum up the total sentiment score.   I tried three different dictionary word list.  The results of all three-sentiment analysis are input into a voting system to determine final sentiment analysis result.  Each method returns a positive, negative or neutral. Which sentiment has the highest voting results is the final sentiment results.  The voting system increased accuracy and reduced variance.  This approach is like an ensemble method.\n",
    "\n",
    "I was surprised by some of the discrepancies in results. I found out list of reviews where review is negative but user rating is 5/5 (false negative), and another list of reviews where sentiment is positive but user rating is 1/5 (false positive). There are total of 9907 out 151,254 reviews or 6.5% of the reviews are in false negative category, and there are 4067 or 2.7% of the reviews are in false positive category. For example, the review 96 list below [1] is a false negative. The user gives a rating of 5 out of 5 and anyone reads it see writer has a positive sentiment to the product. But there are more negative words than positive thus the negative sentiment result. Similarly review 1334 [2] is a false positive example.  In this example, “pretty” was count as a positive sentiment.  My algorithm did not get it right the first time. In the following section I will discuss in detail how I fixed this problem.\n",
    "\n",
    "[1] \"This bottle is a steal, at Whole Foods (at least where I live) a bottle half this size costs twice as much. Before I was also using imitation vanilla, it smells okay but you can smell the fillers. This bottle smells like fresh, pure vanilla. I used it in a lemon poppy seed cake and it was delicious, mild and aromatic! My kitchen smelled WONDERFUL!\"\n",
    "[2] I have had other stevia products before and liked them. This stuff, however, is incredibly nasty. It tastes chemically and toxic. Twice I tried to get myself to drink it and both times I opted to throw out my coffee rather than tolerate the horrid taste. Awful product.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**T3.** Consider a specific outcome you would like to achieve with your sentiment analysis.  That is, determine what sentiment you might want to have assigned to a specific piece of text.  It could be one entry in your corpus, several documents, or the entire corpus. Make changes to the feature space and/or dictionary to achieve that outcome. Show specific results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* I want to print out all negative words in the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.42857142857142855"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hl_output(inputstring):\n",
    "\n",
    "    poscount = 0\n",
    "    negcount = 0\n",
    "    i = 0\n",
    "\n",
    "    for word in inputstring.split():\n",
    "        if i > 0:\n",
    "            prev = inputstring.split().pop(i-1)\n",
    "        else:\n",
    "            prev =\"\"\n",
    "\n",
    "        if HLpos.count(word):\n",
    "            if negate.count(prev):\n",
    "                negcount += 1\n",
    "            elif amplify.count(prev):\n",
    "                poscount +=2\n",
    "            else: \n",
    "                poscount +=1\n",
    "        elif HLneg.count(word):\n",
    "            if negate.count(prev):\n",
    "                poscount += 1\n",
    "            elif amplify.count(prev):\n",
    "                negcount +=2\n",
    "            else:\n",
    "                negcount +=1\n",
    "        i+=1\n",
    "    \n",
    "    if poscount+negcount > 0:\n",
    "        t = float((poscount - negcount)/(poscount+negcount))\n",
    "        \n",
    "    else:\n",
    "        t = 0\n",
    "    return t\n",
    "hl_output(amazonFood.iloc[96]['reviewText'].lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The words \"smells\", \"smell\", \"smells\", \"lemon\", \"smelled\" are negative words in HL dictionary. The product rating is 5/5 so this should be a positive review. The term \"smelled wonderful!\" actual is a positive sentiment. In this case if I first remove punctuations  (\"wonderful\" instead of \"wonderful!\") then I can check if the next word is positive or negative word in the HL dictionary. Below is modified version of hl_send function that flip the result from negative to positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hl_sent_mod(inputstring):\n",
    "\n",
    "    poscount = 0\n",
    "    negcount = 0\n",
    "    i = 0\n",
    "    # Tried different ways 1) remove stop words 2) stemmer 3) remove punctuations.\n",
    "    #words = [word for word in inputstring.split() if word not in set(skl_stopwords)]\n",
    "    #ps = PorterStemmer() #define method \n",
    "    #words = [ps.stem(word) for word in inputstring] \n",
    "    # words = [word for word in inputstring.split()]\n",
    "    words = [word.replace(\"!\", \"\").replace(\".\", \"\").replace(\",\", \"\").replace(\"(\", \"\").replace(\")\", \"\") for word  in inputstring.split()]\n",
    "    for word in words:\n",
    "    \n",
    "        # remove punctuations \n",
    "        word = word\n",
    "        \n",
    "        if i > 0:\n",
    "            prev = words[i-1]\n",
    "        else:\n",
    "            prev = \"\"\n",
    "            \n",
    "        if i < len(words) - 1:\n",
    "            nextWord = words[i+1]\n",
    "        else:\n",
    "            nextWord = \"\"\n",
    "        \n",
    "        \n",
    "        if HLpos.count(word):\n",
    "            if negate.count(prev):\n",
    "                negcount += 1\n",
    "            # if next word is negative then this word is actual negative\n",
    "            elif HLneg.count(nextWord):\n",
    "                negcount += 1\n",
    "            elif amplify.count(prev):\n",
    "                poscount +=2\n",
    "            else: \n",
    "                poscount +=1\n",
    "        elif HLneg.count(word):\n",
    "            if negate.count(prev):\n",
    "                poscount += 1\n",
    "            # if next word is positive then this word is actual positive\n",
    "            elif HLpos.count(nextWord):\n",
    "                poscount += 1\n",
    "            elif amplify.count(prev):\n",
    "                negcount +=2\n",
    "            else:\n",
    "                negcount +=1\n",
    "                \n",
    "        i+=1\n",
    "    #print(poscount)\n",
    "    #print(negcount)\n",
    "    if poscount+negcount > 0:\n",
    "        t = float((poscount - negcount)/(poscount+negcount))\n",
    "        \n",
    "    else:\n",
    "        t = 0\n",
    "    \n",
    "    if t > 0:\n",
    "        tone = \"Positive\"\n",
    "    elif t < 0:\n",
    "        tone = \"Negative\"\n",
    "    else:\n",
    "        tone = \"Neutral\"\n",
    "    \n",
    "    return tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This bottle is a steal, at Whole Foods (at least where I live) a bottle half this size costs twice as much. Before I was also using imitation vanilla, it smells okay but you can smell the fillers. This bottle smells like fresh, pure vanilla. I used it in a lemon poppy seed cake and it was delicious, mild and aromatic! My kitchen smelled WONDERFUL!\n"
     ]
    }
   ],
   "source": [
    "print(amazonFood.iloc[96]['reviewText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negative'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hl_sent(amazonFood.iloc[96]['reviewText'].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hl_sent_mod(amazonFood.iloc[96]['reviewText'].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have had other stevia products before and liked them. This stuff, however, is incredibly nasty. It tastes chemically and toxic. Twice I tried to get myself to drink it and both times I opted to throw out my coffee rather than tolerate the horrid taste. Awful product.\n"
     ]
    }
   ],
   "source": [
    "print(amazonFood.iloc[1334]['reviewText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hl_sent(amazonFood.iloc[1334]['reviewText'].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negative'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hl_sent_mod(amazonFood.iloc[1334]['reviewText'].lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If I re-run all reviews with hl_sent_mod function instead of hl_sent_function. Then calculate the vote results to compare with previous result. We can see the false negative dropped from 6.9% to 5.3%; and the false positive dropped from 2.5% to 2.2%.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amazonFood['hlMod'] = amazonFood.reviewText.apply(lambda x: hl_sent_mod(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>hl</th>\n",
       "      <th>hlMod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just another flavor of Kit Kat but the taste is unique and a bit different.  The only thing that is bothersome is the price.  I thought it was a b...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I bought this on impulse and it comes from Japan,  which amused my family,  all those weird stamps and markings on the package. So that was fun.  ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Really good. Great gift for any fan of green tea! Just so expensive to purchase candy from across the sea.</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I had never had it before, was curious to see what it was like. Smooth, great subtle good flavor. I am ordering more and plan to make it a routine.</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I've been looking forward to trying these after hearing about how popular they were in Japan, and among Kit Kat fans as well. I do not recommend o...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                              reviewText  \\\n",
       "0  Just another flavor of Kit Kat but the taste is unique and a bit different.  The only thing that is bothersome is the price.  I thought it was a b...   \n",
       "1  I bought this on impulse and it comes from Japan,  which amused my family,  all those weird stamps and markings on the package. So that was fun.  ...   \n",
       "2                                             Really good. Great gift for any fan of green tea! Just so expensive to purchase candy from across the sea.   \n",
       "3    I had never had it before, was curious to see what it was like. Smooth, great subtle good flavor. I am ordering more and plan to make it a routine.   \n",
       "4  I've been looking forward to trying these after hearing about how popular they were in Japan, and among Kit Kat fans as well. I do not recommend o...   \n",
       "\n",
       "         hl     hlMod  \n",
       "0  Negative  Negative  \n",
       "1   Neutral  Positive  \n",
       "2   Neutral  Positive  \n",
       "3  Positive  Positive  \n",
       "4  Positive  Positive  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazonFood[['reviewText','hl','hlMod']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amazonFood.to_pickle(\"amazonFood.bin\")\n",
    "amazonFood.to_csv(\"amazonFood.csv\")\n",
    "#amazonFood = pd.read_pickle('amazonFood.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>afinn</th>\n",
       "      <th>hl</th>\n",
       "      <th>vader</th>\n",
       "      <th>vote</th>\n",
       "      <th>hlMod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>A2BYV7S1QP2YIG</td>\n",
       "      <td>B0000531B7</td>\n",
       "      <td>Dr. Oceanfront \"Oceanfront\"</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>Frankly, I was worried about buying these on line because I have heard stories about getting expired food. However, these came in and were very fr...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Good, filling bar...</td>\n",
       "      <td>1397088000</td>\n",
       "      <td>04 10, 2014</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>A14Z6D3IRJ23F7</td>\n",
       "      <td>B00005C2M2</td>\n",
       "      <td>Chris F \"root\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I bought these for a lightweight backpacking snack. From what I remembered as a kid they are very tasty, and I was not disappointed! I've read oth...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Awesome light weigh snack!</td>\n",
       "      <td>1346112000</td>\n",
       "      <td>08 28, 2012</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>AY8UH4COUYMGZ</td>\n",
       "      <td>B0000CDEPD</td>\n",
       "      <td>ttt</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>This bottle is a steal, at Whole Foods (at least where I live) a bottle half this size costs twice as much. Before I was also using imitation vani...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The Only Vanilla I'll Buy From Now On!</td>\n",
       "      <td>1373673600</td>\n",
       "      <td>07 13, 2013</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>AI0BCEWRE04G0</td>\n",
       "      <td>B0000CFLIL</td>\n",
       "      <td>Stoney</td>\n",
       "      <td>[6, 7]</td>\n",
       "      <td>I've used the Mellita Coffee System for about 40 years.  I find the system more convenient than coffee machines.If the power goes off, you can sti...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The Mellita Coffee System</td>\n",
       "      <td>1262822400</td>\n",
       "      <td>01 7, 2010</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>A3JV071KO43Q5X</td>\n",
       "      <td>B0000CNU15</td>\n",
       "      <td>Jeffrey Pittman \"Jeff\"</td>\n",
       "      <td>[8, 9]</td>\n",
       "      <td>My favorite nearby Asian food mart is closing with plans to reopen months from now in a new location.  I need this stuff badly as a staple that mu...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Great stuff, but just STUPID expensive here unfortunately</td>\n",
       "      <td>1338163200</td>\n",
       "      <td>05 28, 2012</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         reviewerID        asin                 reviewerName helpful  \\\n",
       "39   A2BYV7S1QP2YIG  B0000531B7  Dr. Oceanfront \"Oceanfront\"  [1, 1]   \n",
       "61   A14Z6D3IRJ23F7  B00005C2M2               Chris F \"root\"  [0, 0]   \n",
       "96    AY8UH4COUYMGZ  B0000CDEPD                          ttt  [1, 2]   \n",
       "115   AI0BCEWRE04G0  B0000CFLIL                       Stoney  [6, 7]   \n",
       "143  A3JV071KO43Q5X  B0000CNU15       Jeffrey Pittman \"Jeff\"  [8, 9]   \n",
       "\n",
       "                                                                                                                                                reviewText  \\\n",
       "39   Frankly, I was worried about buying these on line because I have heard stories about getting expired food. However, these came in and were very fr...   \n",
       "61   I bought these for a lightweight backpacking snack. From what I remembered as a kid they are very tasty, and I was not disappointed! I've read oth...   \n",
       "96   This bottle is a steal, at Whole Foods (at least where I live) a bottle half this size costs twice as much. Before I was also using imitation vani...   \n",
       "115  I've used the Mellita Coffee System for about 40 years.  I find the system more convenient than coffee machines.If the power goes off, you can sti...   \n",
       "143  My favorite nearby Asian food mart is closing with plans to reopen months from now in a new location.  I need this stuff badly as a staple that mu...   \n",
       "\n",
       "     overall                                                    summary  \\\n",
       "39       5.0                                       Good, filling bar...   \n",
       "61       5.0                                 Awesome light weigh snack!   \n",
       "96       5.0                     The Only Vanilla I'll Buy From Now On!   \n",
       "115      5.0                                  The Mellita Coffee System   \n",
       "143      5.0  Great stuff, but just STUPID expensive here unfortunately   \n",
       "\n",
       "     unixReviewTime   reviewTime     afinn        hl    vader      vote  \\\n",
       "39       1397088000  04 10, 2014  Positive   Neutral  Neutral  Positive   \n",
       "61       1346112000  08 28, 2012  Positive   Neutral  Neutral  Positive   \n",
       "96       1373673600  07 13, 2013  Positive  Negative  Neutral   Neutral   \n",
       "115      1262822400   01 7, 2010  Positive   Neutral  Neutral   Neutral   \n",
       "143      1338163200  05 28, 2012  Negative  Negative  Neutral  Negative   \n",
       "\n",
       "        hlMod  \n",
       "39   Positive  \n",
       "61   Positive  \n",
       "96   Positive  \n",
       "115  Positive  \n",
       "143  Positive  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingDoesnotMatch = amazonFood[(amazonFood['overall'] == 5) & ('Positive' != amazonFood['hl']) & ('Positive' == amazonFood['hlMod'])]\n",
    "ratingDoesnotMatch.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([   354,    355,    361,    470,    544,    565,    613,    757,\n",
       "               827,   1256,\n",
       "            ...\n",
       "            147856, 147963, 148479, 148810, 149402, 149632, 149693, 150438,\n",
       "            150788, 150968],\n",
       "           dtype='int64', length=675)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingDoesnotMatch = amazonFood[(amazonFood['overall'] == 1) & ('Negative' != amazonFood['hl']) & ('Negative' == amazonFood['hlMod'])]\n",
    "ratingDoesnotMatch.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10465\n",
      "0.0691882528726513\n",
      "3835\n",
      "0.02535470136326973\n"
     ]
    }
   ],
   "source": [
    "ratingDoesnotMatch = amazonFood[(amazonFood['overall'] == 5) & ('Positive' != amazonFood['hl'])]\n",
    "print(len(ratingDoesnotMatch))\n",
    "print(len(ratingDoesnotMatch)/len(amazonFood))\n",
    "ratingDoesnotMatch = amazonFood[(amazonFood['overall'] == 1) & ('Negative' != amazonFood['hl'])]\n",
    "print(len(ratingDoesnotMatch))\n",
    "print(len(ratingDoesnotMatch)/len(amazonFood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8118\n",
      "0.05367130786623825\n",
      "3354\n",
      "0.022174620175334205\n"
     ]
    }
   ],
   "source": [
    "ratingDoesnotMatch = amazonFood[(amazonFood['overall'] == 5) & ('Positive' != amazonFood['hlMod'])]\n",
    "print(len(ratingDoesnotMatch))\n",
    "print(len(ratingDoesnotMatch)/len(amazonFood))\n",
    "ratingDoesnotMatch = amazonFood[(amazonFood['overall'] == 1) & ('Negative' != amazonFood['hlMod'])]\n",
    "print(len(ratingDoesnotMatch))\n",
    "print(len(ratingDoesnotMatch)/len(amazonFood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Recall I used three different dictionaries and use a voting system to get final results.  Use the new hl_sent_mod function also improved the voting result.  It reduced the false negative from 6.5% to 5.0%, and false positive from 2.7% to 2.6%.  The new hl_sent_mod function improves the accuracy judging using all 151,254 Amazon food reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7626\n",
      "0.050418501328890475\n",
      "3982\n",
      "0.026326576487233397\n"
     ]
    }
   ],
   "source": [
    "vote_sent(amazonFood)\n",
    "ratingDoesnotMatch = amazonFood[(amazonFood['overall'] == 5) & ('Positive' != amazonFood['vote'])]\n",
    "print(len(ratingDoesnotMatch))\n",
    "print(len(ratingDoesnotMatch)/len(amazonFood))\n",
    "ratingDoesnotMatch = amazonFood[(amazonFood['overall'] == 1) & ('Negative' != amazonFood['vote'])]\n",
    "print(len(ratingDoesnotMatch))\n",
    "print(len(ratingDoesnotMatch)/len(amazonFood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.**  Write a short description of the exercise and the outcome.  Make sure your answer is no longer than three paragraphs, and should at minimum answer these questions:\n",
    "* What outcome did you choose?  Why?\n",
    "* How did you change the dictionary to achieve that outcome?\n",
    "* How would you explain (justify, rationalize) those changes if necessary?\n",
    "Audience: general – management or non-technical staff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HL function gives a negative sentiment on review 96. However, the user's product rating is 5/5 so this should be a positive review.  The review is positive from reading it.  But computer does not understand that.  So, I want to change the algorithm to fix it.  It turns out words \"smell\" \"smells\", \"smell\", \"smells\", \"smelled\" are all negative words in the HL dictionary. But this is a food review therefore \"smell\" is a normal word.  And in actual text \"smelled wonderful\", \"smelled okay\" are positive or at least neutral. First I realized “smelled wonderful!” did not count because the punctuation was not removed.  By removing the punctuations from \"delicious,\" \"aromatic!\" and \"wonderful!\", these words will be matched with positive words in the dictionary, this improves review sentiment to more positive.  Then, we would not miss some important positive or negative words.  But the text still has more negative words than positive words.  At first I wanted to remove \"smell\" all other \"smells\", \"smell\", \"smells\", \"smelled\" from the HL dictionary but I decided to not to do it.  This is because \"smelled\" can still be a negative sentiment in other cases.  For example, \"something smelled\" like \"something stinks\" has a negative connotation. I do not want to change the rule here for one text.\n",
    "\n",
    "I thought we were already looking at previous word why not look at next word too.  By looking at both previous and next words we can get more accurate results.  For words that are both positive and negative sentiment, if the next word is opposite sentiment, then we do not count it.  I modified the function to look at the next word.  After making this change this review’s sentiment result changed from negative to positive.  But this is just one example, how can I justify making a change without testing it on more data?  I can justify the changes after seeing the reductions in false negative and false positive numbers from validation on all 151,254 reviews.  It reduced the false negative from 6.9% to 5.3%, and false positive from 2.5% to 2.2%.  The improvement is because false positive term “smelled wonderful” is positive even though “smelled” is a negative word; false negative term “incredibly nasty” is now negative even though “incredible” is a positive word.  The improvement on false positive is less because there are less terms like “incredibly nasty”, “pretty dreadful”, and “super disgusting” (positive word with next word is negative).  \n",
    "\n",
    "We only look at one previous word, in the future I want to look at two previous words: “no longer”, “no more”.  “no longer good” should be a negative sentiment.  There are many two words term we can consider.  I did not include them in my function but it certainly can improve result even more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.** Data science is all about finding patterns in the data.  You have just been asked to decide on a pattern before finding it. Write a short description of how the easy or difficult it was to arrive at a predetermined conclusion.  How difficult was it to justify? What are the ethical issues involved, if any? What is your role as a data scientist? \n",
    "Audience: general – management or non-technical staff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can see it would be an ethical concern. In text mining, it is easy to change the dictionary, replace the text, or add or remove or use different list of stop words.  The decision would affect the outcome easily.  Computer does not understand the text, it only understands what human tells it what to do using an algorithm.  The difficult part is to do the right thing and have good rational on making a change.  \n",
    "\n",
    "It depends on the situation on how to tweak the rules. In my view a data scientist must consider carefully when changing a rule or data.  First, we need to ask a lot of questions first.  Is this a very specific case or unique term and cannot be used in other way?  For example, “NYC” replaces “New York City”, or “BofA” for “Bank of America”.  However, if this change has broad implications in general situations, then we must find a good rule that fit the most of the cases.  If there is less data, we must gather larger data set. The results must be validated using training and validation data.  \n",
    "\n",
    "We also want to look at more existing research and find whether the pattern is a good fixture based on research.  I can see domain knowledge is also important.  The text documents on financial reporting is very different from movie reviews.  So, I would think as data scientists we need to research domain specific knowledge before making a change and arriving a conclusion.  In BI class we talked more about this, [Spurious correlations site](http://www.tylervigen.com/spurious-correlations) has many examples of incorrect patterns to avoid."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
